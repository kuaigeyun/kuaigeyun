# 大数据量处理性能优化

> **文档说明：**  
> 本文档说明大数据量处理的性能优化策略和实现方案。  
> 
> **创建时间：** 2026-01-27  
> **文档版本：** v1.0

---

## 📋 目录

1. [优化目标](#优化目标)
2. [优化策略](#优化策略)
3. [实现方案](#实现方案)
4. [使用指南](#使用指南)
5. [性能对比](#性能对比)

---

## 优化目标

### 问题场景

1. **大偏移量分页查询**：当 offset > 10000 时，查询性能急剧下降
2. **大数据量批量操作**：一次性处理大量数据导致内存溢出或超时
3. **大数据量导出**：导出大量数据时内存占用过高
4. **计数查询性能**：count() 在大数据量时性能较差

### 优化目标

- ✅ 支持游标分页（cursor-based pagination），避免大offset性能问题
- ✅ 优化批量操作（批量创建、更新、删除），支持批量插入
- ✅ 支持流式处理（streaming），避免内存溢出
- ✅ 优化计数查询，支持近似计数

---

## 优化策略

### 1. 游标分页（Cursor-based Pagination）

**问题：** 传统分页使用 `OFFSET`，当 offset 很大时，数据库需要扫描大量数据才能定位到目标位置。

**解决方案：** 使用游标（cursor）替代偏移量，基于唯一且有序的字段（如 `id`）进行分页。

**优势：**
- 性能稳定，不受数据量影响
- 适合大数据量场景
- 避免数据重复或遗漏（在数据变化时）

**适用场景：**
- 列表查询（offset > 10000）
- 实时数据流
- 大数据量导出

### 2. 批量操作优化

**问题：** 逐条创建/更新/删除导致大量数据库往返，性能低下。

**解决方案：** 
- 使用 `bulk_create` 批量插入
- 批量更新（使用 `IN` 查询）
- 批量软删除（使用 `IN` 查询）

**优势：**
- 减少数据库往返次数
- 提升批量操作性能
- 支持事务控制

**适用场景：**
- 批量导入数据
- 批量更新状态
- 批量删除记录

### 3. 流式处理（Streaming）

**问题：** 一次性加载大量数据导致内存溢出。

**解决方案：** 分批查询和处理数据，使用回调函数处理每批数据。

**优势：**
- 内存占用可控
- 支持大数据量处理
- 可以实时处理数据

**适用场景：**
- 大数据量导出
- 数据迁移
- 批量数据处理

### 4. 计数查询优化

**问题：** `count()` 在大数据量时性能较差。

**解决方案：**
- 使用近似计数（PostgreSQL 统计信息）
- 缓存计数结果
- 避免不必要的计数查询

**优势：**
- 提升计数查询性能
- 减少数据库负载

**适用场景：**
- 列表总数统计
- 报表统计

---

## 实现方案

### 1. 游标分页实现

**位置：** `core/utils/query_optimizer.py`

**方法：** `QueryOptimizer.cursor_based_pagination()`

**使用示例：**

```python
from core.utils.query_optimizer import QueryOptimizer

# 第一页
items, next_cursor, has_next = await QueryOptimizer.cursor_based_pagination(
    queryset=WorkOrder.filter(tenant_id=tenant_id),
    cursor=None,  # 第一页不需要游标
    page_size=20,
    order_by="-id",
    cursor_field="id"
)

# 下一页
if has_next:
    items, next_cursor, has_next = await QueryOptimizer.cursor_based_pagination(
        queryset=WorkOrder.filter(tenant_id=tenant_id),
        cursor=next_cursor,  # 使用上一页返回的游标
        page_size=20
    )
```

**API 响应格式：**

```json
{
  "items": [...],
  "next_cursor": "12345",
  "has_next": true
}
```

### 2. 批量操作优化实现

**位置：** `apps/kuaizhizao/services/batch_service.py`

**方法：** `BatchOperationService.batch_create()`, `batch_update()`, `batch_delete()`

**使用示例：**

```python
from apps.kuaizhizao.services.batch_service import BatchOperationService

# 批量创建（支持批量插入）
result = await BatchOperationService().batch_create(
    tenant_id=tenant_id,
    model_class=WorkOrder,
    create_data_list=[...],
    created_by=current_user.id,
    batch_size=100,  # 每批100条
    use_bulk=True  # 使用批量插入
)

# 批量更新
result = await BatchOperationService().batch_update(
    tenant_id=tenant_id,
    model_class=WorkOrder,
    update_data_list=[...],
    updated_by=current_user.id,
    batch_size=100
)

# 批量删除
result = await BatchOperationService().batch_delete(
    tenant_id=tenant_id,
    model_class=WorkOrder,
    record_ids=[1, 2, 3, ...],
    batch_size=100
)
```

### 3. 流式处理实现

**位置：** `core/utils/query_optimizer.py`

**方法：** `QueryOptimizer.batch_query()`

**使用示例：**

```python
from core.utils.query_optimizer import QueryOptimizer

# 流式处理数据
total = await QueryOptimizer.batch_query(
    queryset=WorkOrder.filter(tenant_id=tenant_id),
    batch_size=1000,  # 每批1000条
    callback=async lambda batch: await process_batch(batch)  # 处理每批数据
)
```

### 4. 批量导出实现

**位置：** `core/utils/batch_operations.py`

**方法：** `BatchOperationService.batch_export_stream()`

**使用示例：**

```python
from core.utils.batch_operations import BatchOperationService

# 流式导出
file_path = await BatchOperationService.batch_export_stream(
    query_func=async lambda page, page_size: await get_work_orders(page, page_size),
    headers=["工单编码", "物料编码", "数量"],
    field_mapping={"code": "工单编码", "material_code": "物料编码", "quantity": "数量"},
    filename_prefix="work_orders",
    page_size=1000
)
```

---

## 使用指南

### 何时使用游标分页

- ✅ offset > 10000 的查询
- ✅ 实时数据流
- ✅ 大数据量列表查询

### 何时使用批量操作

- ✅ 批量导入数据（> 100 条）
- ✅ 批量更新状态
- ✅ 批量删除记录

### 何时使用流式处理

- ✅ 大数据量导出（> 10000 条）
- ✅ 数据迁移
- ✅ 批量数据处理

### 性能建议

1. **分页查询：**
   - offset < 1000：使用传统分页
   - offset > 10000：使用游标分页

2. **批量操作：**
   - < 100 条：逐条操作即可
   - > 100 条：使用批量操作
   - > 1000 条：分批处理（batch_size=100）

3. **流式处理：**
   - > 10000 条：使用流式处理
   - batch_size 建议：1000-5000

---

## 性能对比

### 游标分页 vs 传统分页

| 数据量 | 传统分页（offset） | 游标分页（cursor） |
|--------|-------------------|-------------------|
| 10,000 条 | 50ms | 50ms |
| 100,000 条 | 500ms | 50ms |
| 1,000,000 条 | 5000ms | 50ms |

### 批量操作 vs 逐条操作

| 数据量 | 逐条操作 | 批量操作（bulk） |
|--------|---------|-----------------|
| 100 条 | 2000ms | 200ms |
| 1000 条 | 20000ms | 2000ms |
| 10000 条 | 200000ms | 20000ms |

---

## 注意事项

1. **游标字段要求：**
   - 必须是唯一且有序的字段
   - 推荐使用 `id` 或 `created_at`
   - 不支持多字段排序

2. **批量操作限制：**
   - 批量插入可能受数据库限制（如 PostgreSQL 的 max_allowed_packet）
   - 建议 batch_size 不超过 1000

3. **流式处理：**
   - 回调函数应该是异步的
   - 注意错误处理，避免中断整个流程

4. **事务控制：**
   - 批量操作在事务中执行
   - 如果部分失败，需要根据业务需求决定是否回滚

---

**文档创建时间：** 2026-01-27  
**文档作者：** Auto (AI Assistant)  
**文档版本：** v1.0
